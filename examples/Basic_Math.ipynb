{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujvVmMa0btJb",
        "outputId": "e59a07a1-b327-485c-b9f7-c5455b62ec96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for gofannon (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/The-AI-Alliance/gofannon.git --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "import json\n",
        "# Create an OpenAI client with your deepinfra token and endpoint\n",
        "openai = OpenAI(\n",
        "    api_key=userdata.get('deepinfra'),\n",
        "    base_url=\"https://api.deepinfra.com/v1/openai\",\n",
        ")\n",
        "\n",
        "from gofannon.basic_math.exponents import Exponents\n",
        "from gofannon.basic_math.multiplication import Multiplication\n",
        "from gofannon.basic_math.division import Division\n",
        "from gofannon.basic_math.addition import Addition\n",
        "from gofannon.basic_math.subtraction import Subtraction\n",
        "\n",
        "\n",
        "tool_list =  [F() for F in [Addition, Subtraction, Multiplication, Division, Exponents]]\n",
        "tool_map = {f.name: f.fn for f in tool_list}\n",
        "tool_desc_list = [f.definition for f in tool_list]"
      ],
      "metadata": {
        "id": "JBUKOfaFcSlF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here is the user request\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What is 12*457?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# let's send the request and print the response\n",
        "response = openai.chat.completions.create(\n",
        "    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
        "    messages=messages,\n",
        "    tools=tool_desc_list,\n",
        "    tool_choice=\"auto\",\n",
        ")\n",
        "tool_calls = response.choices[0].message.tool_calls\n",
        "for tool_call in tool_calls:\n",
        "    print(tool_call.model_dump())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RACg4DbIbyNH",
        "outputId": "a90d5fd2-4985-46d3-c3af-f2c823fdd51e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'call_YDinihVWmM4fgG0G7ZrD8JoI', 'function': {'arguments': '{\"num1\": 12.0, \"num2\": 457.0}', 'name': 'multiplication'}, 'type': 'function'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extend conversation with assistant's reply\n",
        "messages.append(response.choices[0].message)\n",
        "\n",
        "for tool_call in tool_calls:\n",
        "  function_name = tool_call.function.name\n",
        "  function_args = json.loads(tool_call.function.arguments)\n",
        "  function_response = tool_map[function_name](**function_args)\n",
        "\n",
        "  # extend conversation with function response\n",
        "  messages.append({\n",
        "      \"tool_call_id\": tool_call.id,\n",
        "      \"role\": \"tool\",\n",
        "      \"content\": function_response,\n",
        "  })\n",
        "\n",
        "\n",
        "# get a new response from the model where it can see the function responses\n",
        "second_response = openai.chat.completions.create(\n",
        "  model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
        "  messages=messages,\n",
        "  tools=tool_desc_list,\n",
        "  tool_choice=\"auto\",\n",
        ")\n",
        "\n",
        "print(second_response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd3QAvMZeBfS",
        "outputId": "31795ea4-88c1-436f-a936-165a9921ca61"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The result of the multiplication is 5484.\n"
          ]
        }
      ]
    }
  ]
}