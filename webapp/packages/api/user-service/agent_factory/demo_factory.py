# webapp/packages/api/user-service/agent_factory/demo_factory.py
from .prompts import how_to_build_demo_app_template
from models.demo import GenerateDemoCodeRequest, GenerateDemoCodeResponse, DeployedApi
import json

from services.llm_service import call_llm
from config.provider_config import PROVIDER_CONFIG

def _format_api_docs(apis: list[DeployedApi]) -> str:
    """Formats the API information into a markdown string for the prompt."""
    if not apis:
        return "No APIs selected."
    
    docs_parts = []
    for api in apis:
        input_schema_str = json.dumps(api.input_schema, indent=2)
        output_schema_str = json.dumps(api.output_schema, indent=2)
        
        doc = f"""### API: `{api.friendly_name}`
**Endpoint**: `POST /rest/{api.friendly_name}`
**Description**: {api.description}

**Input Schema (Request Body JSON):**
```json
{input_schema_str}
```

**Output Schema (Response Body JSON):**
```json
{output_schema_str}
```
"""
        docs_parts.append(doc)
    
    return "\n\n".join(docs_parts)


async def generate_demo_code(request: GenerateDemoCodeRequest) -> GenerateDemoCodeResponse:
    """
    Generates HTML, CSS, and JS for a demo app based on a user prompt and selected APIs.
    """
    api_docs = _format_api_docs(request.selected_apis)
    
    system_prompt = how_to_build_demo_app_template.format(
        api_docs=api_docs,
        user_prompt=request.user_prompt
    )

    messages = [
        {"role": "system", "content": "You are a helpful assistant that generates web application code in JSON format."},
        {"role": "user", "content": system_prompt}
    ]

    model = request.composer_model_config.model
    provider = request.composer_model_config.provider
    config = request.composer_model_config.parameters.copy()
    if provider == "openai":
        config['response_format'] = { "type": "json_object" }

    # Build tools list from config
    built_in_tools = []
    model_tool_config = PROVIDER_CONFIG.get(provider, {}).get("models", {}).get(model, {}).get("built_in_tools", [])
    if request.built_in_tools:
        for tool_id in request.built_in_tools:
            tool_conf = next((t for t in model_tool_config if t["id"] == tool_id), None)
            if tool_conf:
                built_in_tools.append(tool_conf["tool_config"])

    response_content, thoughts = await call_llm(
        provider=provider,
        model=model,
        messages=messages,
        parameters=config,
        tools=built_in_tools if built_in_tools else None
    )

    try:
        # Clean up potential markdown
        if response_content.strip().startswith("```json"):
            response_content = response_content.strip()[len("```json"):].strip()
        if response_content.strip().endswith("```"):
            response_content = response_content.strip()[:-len("```")].strip()
            
        code_json = json.loads(response_content)
        
        # Ensure the response includes the thoughts
        response_obj = GenerateDemoCodeResponse(**code_json)
        response_obj.thoughts = thoughts
        return response_obj
    except (json.JSONDecodeError, TypeError) as e:
        print(f"Error parsing demo code JSON from LLM: {e}")
        print(f"LLM response was: {response_content}")
        raise ValueError(f"Could not parse the code generated by the model. Response:\n{response_content}")